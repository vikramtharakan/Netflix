{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was just practice to remember how to even lemmatie a sentance in the first place. No need to look at this, this was solely for my reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package abc to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading necessary libraries. We don't actually use the sent_tokenize in this one, but it works the same as the\n",
    "# word_tokenize function\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('abc')\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my dumb Example sentance, The Fat man wAs running up the hill so fAST, that he had to drive homE. S20 beam off\n",
      "\n",
      "\n",
      "This is the tokenized sentance: ['this', 'is', 'my', 'dumb', 'Example', 'sentance', ',', 'The', 'Fat', 'man', 'wAs', 'running', 'up', 'the', 'hill', 'so', 'fAST', ',', 'that', 'he', 'had', 'to', 'drive', 'homE', '.', 'S20', 'beam', 'off']\n",
      "\n",
      "\n",
      "This is the list of lemmatized words, with no white space, and all lower case: ['this', 'is', 'my', 'dumb', 'example', 'sentance', ',', 'the', 'fat', 'man', 'was', 'running', 'up', 'the', 'hill', 'so', 'fast', ',', 'that', 'he', 'had', 'to', 'drive', 'home', '.', 's20', 'beam', 'off']\n",
      "\n",
      "\n",
      "Final list of filtered words: ['dumb', 'example', 'sentance', 'fat', 'man', 'running', 'hill', 'fast', 'drive', 'home', 'beam']\n"
     ]
    }
   ],
   "source": [
    "# Stupid example sentance\n",
    "example_sent = \"this is my dumb Example sentance, The Fat man wAs running up the hill so fAST, that he had to drive homE. S20 beam off\"\n",
    "print(example_sent)\n",
    "print('\\n')\n",
    "\n",
    "# List of stop words from NLTK. Will get rid of dumb words like \"this\", \"my\", \"the\", etc... Keeps only important words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Breaks the sentance up into\n",
    "tokenized_sent = word_tokenize(example_sent)\n",
    "print(\"This is the tokenized sentance: \" + str(tokenized_sent) + \"\\n\\n\")\n",
    "\n",
    "# Creates a lematizer function that will truncate words to their \"original form\". Thus drive, drove, and driven should \n",
    "# all ideally be the same word. Not 100% accurate\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Goes through and lematizes each word. Additionally, it gets rid of any potential white space, and makes the whol\n",
    "# word lower case. Adds all these word to a list named \"clean_words\"\n",
    "clean_words = []\n",
    "for word in tokenized_sent:\n",
    "    clean_words.append(lemmatizer.lemmatize(word).lower().strip())\n",
    "print(\"This is the list of lemmatized words, with no white space, and all lower case: \" + str(clean_words) + \"\\n\\n\")\n",
    "\n",
    "# The text is now tokenized and lemmatized, lets take out all the stop words (done in first part of next line).\n",
    "# The last part w.isalpha() makes sure that the \"word\" is contains all letters. Thus the \"word\" cannot be a comma or\n",
    "# a number for example\n",
    "filtered_words = [w for w in clean_words if w not in stop_words and w.isalpha()]\n",
    "print(\"Final list of filtered words: \" + str(filtered_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The first example doesn't take care of examples where words include numbers. This will be important for entries such as \"S20 injector yada yada ...\" because S20 has both numerical and alphabetical characters and we want to keep that. Let's try and get around that with the use of the following line\n",
    "\n",
    "<center> text = re.sub(r\"[^a-zA-Z0-9]\",\" \", text) </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my dumb Example sentance, The Fat man wAs running up the hill so fAST, that he had to drive homE. S20 beam off\n",
      "\n",
      "\n",
      "This is the tokenized sentance: ['this', 'is', 'my', 'dumb', 'Example', 'sentance', 'The', 'Fat', 'man', 'wAs', 'running', 'up', 'the', 'hill', 'so', 'fAST', 'that', 'he', 'had', 'to', 'drive', 'homE', 'S20', 'beam', 'off']\n",
      "\n",
      "\n",
      "This is the list of lemmatized words, with no white space, and all lower case: ['this', 'is', 'my', 'dumb', 'example', 'sentance', 'the', 'fat', 'man', 'was', 'running', 'up', 'the', 'hill', 'so', 'fast', 'that', 'he', 'had', 'to', 'drive', 'home', 's20', 'beam', 'off']\n",
      "\n",
      "\n",
      "Final list of filtered words: ['dumb', 'example', 'sentance', 'fat', 'man', 'running', 'hill', 'fast', 'drive', 'home', 's20', 'beam']\n"
     ]
    }
   ],
   "source": [
    "# Stupid example number 2\n",
    "example_sent = \"this is my dumb Example sentance, The Fat man wAs running up the hill so fAST, that he had to drive homE. S20 beam off\"\n",
    "print(example_sent)\n",
    "print('\\n')\n",
    "\n",
    "# List of stop words from NLTK. Will get rid of dumb words like \"this\", \"my\", \"the\", etc... Keeps only important words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Breaks the sentance up into\n",
    "no_punctuation_sent = re.sub(r\"[^a-zA-Z0-9]\",\" \", example_sent)\n",
    "tokenized_sent = word_tokenize(no_punctuation_sent)\n",
    "print(\"This is the tokenized sentance: \" + str(tokenized_sent) + \"\\n\\n\")\n",
    "\n",
    "# Creates a lematizer function that will truncate words to their \"original form\". Thus drive, drove, and driven should \n",
    "# all ideally be the same word. Not 100% accurate\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Goes through and lematizes each word. Additionally, it gets rid of any potential white space, and makes the whol\n",
    "# word lower case. Adds all these word to a list named \"clean_words\"\n",
    "clean_words = []\n",
    "for word in tokenized_sent:\n",
    "    clean_words.append(lemmatizer.lemmatize(word).lower().strip())\n",
    "print(\"This is the list of lemmatized words, with no white space, and all lower case: \" + str(clean_words) + \"\\n\\n\")\n",
    "\n",
    "# The text is now tokenized and lemmatized, lets take out all the stop words (done in first part of next line).\n",
    "# The last part w.isalpha() makes sure that the \"word\" is contains all letters. Thus the \"word\" cannot be a comma or\n",
    "# a number for example\n",
    "filtered_words = [w for w in clean_words if w not in stop_words]\n",
    "print(\"Final list of filtered words: \" + str(filtered_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
